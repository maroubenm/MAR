
\chapter{Hand gesture Recognition} \label{hgr}




\section{Hand tracking  And Detection}

\subsection{Hand segmentation based on depth information :} \label{depth}

The First step we need to accomplish in order to start to work with the depth data, is to decide which pixels are we going to take into account to carry on the tracking. The Kinect can catch the distance of the points which are visible to the camera, between the values \textbf{minDistance and maxDistance} (\ref{fig:cam8}).

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{img/mindistance.png}
\caption{Relative depth thresholding }
\label{fig:cam8}
\end{figure}

We are going to base our choice on the object’s proximity to the Kinect. We can choose if a pixel is near based on its depth  The lesser the depth the more likely it’s the hand region .\\

\textbf{ Absolute depth}: A pixel is near if its depth is lesser than a constant value, that means that the pixel is between \textbf{minDistance }and a predeﬁned value which is greater than \textbf{minDistance }and lesser than \textbf{maxDistance}.\\

\textbf{Relative depth }:First of all,  we calculate the minimum depth, and  base on that depth we select a maximum depth (by adding a constant value to the minimum depth). So, if the depth is between these two values, the pixel is near. This method allow us to have greater mobility, because it does not force us to stay in the same position the whole time. 
\\


\textbf{In the figure \ref{fig:cam8}} the minimum depth is \textbf{minDistance} and we add a tolerance of\textbf{  t= 2cm }to it, so every point between \textbf{minDistance and minDistance + t }  will be accepted as near.\\

After selecting a method, we must select the resolution of the depth image. It is possible to select it from three different  options 80x60, 320x240 or 640x480. The best option is obviously the 640x480 resolution, but in order to improve the efﬁciency of the code, we can choose the 271 x 211 resolution, because it gives us enough deﬁnition to distinguish the contour  of the hands. \\This choice greatly reduce the number of operations, and consequently, it will improve the efﬁciency of the code.

\subsection{Hand Coordinates in depth space and color space }
A normal color camera can be used for sign language recognition but the problem comes when we want to use it for real time applications, in this case we have to track the hand position first and then we can go for recognition but implementing the hand tracking will involve complex algorithms which will make the overall system  computationally heavy, but with the kinect depth camera this task becomes easy as it has a skeleton tracking capability by using color and depth images so we can track the approximate hand position using the skeleton data, there are 20 joints that can be tracked these joints numbers are shown in \ref{table:t1}\\

\begin{table}[h!]
\centering

\begin{tabu} to 0.6\textwidth { | X[l] | X[r] | }

 \hline
 SpineMid = 1  & Neck = 2    \\
 \hline 
 Head = 3  &  ShoulderLeft = 4    \\
\hline
  ElbowLeft = 5   &  WristLeft = 6    \\
\hline

 HandLeft =	7  &   ShoulderRight = 8    \\
\hline
 
ElbowRight = 9  &  WristRight = 10   \\
\hline
 
 HandRight = 11  &  HipLeft = 12   \\
\hline
 
 KneeLeft = 13  &  AnkleLeft = 14   \\
\hline
 
FootLeft = 15  &  HipRight = 16   \\
\hline
 
 KneeRight = 17  & AnkleRight = 18     \\
\hline

FootRight = 19   & SpineShoulder = 20     \\

\hline

 \end{tabu}
\caption{JointType Enumeration}
 \label{table:t1} 

 \end{table}

in the following figure \ref{fig:cam9} we will show the hand position both in color and depth space :

\begin{figure}[H]
\centering
\includegraphics[width= 1.0\textwidth]{img/colorvsdepth.PNG}

\caption {Hand position's in both color image and depth imge .
\label{fig:cam9}}

\end{figure}

\subsection{Hand isolation using object connectivity }

After depth thresholding we notice that there are regions that do not belong to the hand , such as what appears to be one of my  arms or some furniture or my body when my hand is closer to my body like shown in the figure  \ref{fig:cam10} . These objects just happen to be on the same depth layer as my arm and hand this means that the Distance between these objects and the Kinect lies  between \textbf{[minDistance ,minDistance +2cm]} .
\newline

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{img/depththresholding.png}
\caption{the depth of the body is in  the range [minDistance ,minDistance +2cm]}
\label{fig:cam10}
\end{figure}


the approach used in this project  is to realize that most of the times, hands are not connected to any other object after the thresholding . We already know that the center region belongs to the hand from skeleton tracking , so we can simply apply`` floodfill ()`` offered by OpenCV to find all the connected image regions.\newline

\textbf{ Definition and explanation of the algorithm used in floodfill () }

\begin{algorithm}

\caption{Algorithm of Flood Fill }

\textbf{function}  Flood-fill(node,target\_color,replacement\_color) 
\newline
\If{target\_color  ==  replacement\_color }
    {
     return
    }
\newline
\If{color of node \neq target\_color }
    {
     return
    }
Set the color of node to replacement$\_$color 


\textbf{Flood-fill}(one step to the west of node , target\_color , replacement\_color)\\
\textbf{Flood-fill}(one step to the east of node , target\_color , replacement\_color)\\
\textbf{Flood-fill}(one step to the north of node , target\_color , replacement\_color)\\
\textbf{Flood-fill}(one step to the south of node , target\_color , replacement\_color)
\newline

\textbf{return.} 
\end{algorithm}
\newline

with this simple algorithm we can seperate objects from each other and therefor the following  Results : \\
 
 \begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{img/finalresult.png}
\caption{result after applying connectivity check for one hand  }
\label{fig:cam11}
\end{figure}


the same process done with the other hand , Making a \textbf{OR } operation to the two images we end up having two hands isolated like shown in fig \ref{fig:twohd}

 \begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{img/twohands.jpg}
\caption{result after applying connectivity for both hands  }
\label{fig:twohd}
\end{figure}

now after finishing  the prepocessing phase , Since we are taking a resolution of 271 $\times$ 211 Pixels , to make sure the hands are being tracked we make a bounding box that is basically related to the left hand Position , this was made under the assumption that sign language  usually performed with two hands close to each other , \\
bounding box steps : \\

\begin{itemize}
\item  Use LEFT hand location provided by SDK Skeleton  tracker (Px, Py)
 \item Crop to box around hand using depth at (Px, Py)
    \item scale = $\frac{20000.0f}{Depth(px,py)}$ 
    \begin{enumerate}
    \item Constant 20000.0f  found empirically
    \item Proportional to the area of bounding box 
    \end{enumerate}
\item Top left Pixel  = ( Px -10 * Scale * 0.5  , Py - 10*Scale * 0.5)
\begin{enumerate}
    \item This formula scales an initial 10x10 box centered at (Px, Py) to
the appropriate dimensions based on the depth 
    \item Constants 0.5  take into account that most hand
silhouettes are more tall than wide 
    \end{enumerate}
   \item Width = Height = 10 * Scale 
\end{itemize}

in the next section we will build the recognition System  .

\section{ gesture recognition using Surf  Features  and SVM  classifier }

\subsection{ Classification Using Bag-Of-Words Model}

Computer Vision has achieved some success in the case of specific problems, like detecting faces in an image. But it has still not satisfactorily solved the problem for the general case where we have random objects in different positions and orientations. \textbf{Bag-of-words (BoW) } model is one of the most popular approaches in this field, and many modern approaches are based on top of this.
The idea is very similar to a Dictionary of keypoints or local interest points of an Image or a Datasets of Images.We represent an object as a bag of \textbf{visual words} Fig \ref{fig:coodbook}.\\
\begin{wrapfigure}{l}{0.25\textwidth}
\includegraphics[width=0.25\textwidth]{img/codebook.jpg}
\caption{Example of Visual words in a codebook}
\label{fig:coodbook}
\end{wrapfigure}
These visual words are basically important points in the images. These points are \textbf{features}, and they are discriminative. What this means is that a big patch of monotonic region is not considered to be a feature because it doesn’t give us much information. This is in contrast with a sharp corner or a unique color combination, where we get a lot of information about the image. We can use the BoW model for image classification by constructing a large vocabulary of many visual words and represent each image as a histogram of the frequency words that are in the image. To actually use BoW for image classification, we need to extract these features, generate a codebook, and then generate a histogram. in order to get these features we are going to Use Local Features Descriptors like ( SIFT , SURF ,ORB )\\
After this step, each image is a collection of vectors of the same dimension, where the order of different vectors is\textbf{ not important} , but the sets vary in cardinality and lack meaningful ordering. This creates difficulties for learning methods (e.g., classifiers) that require feature vectors of fixed dimension as input.\\ Use of the \textbf{vector quantization (VQ)} technique clusters the keypoint descriptors in their feature space into a large number of clusters using \textbf{the K-means } clustering algorithm and encodes each keypoint by the index of the cluster to which it belongs.\\ Each cluster as a visual word that represents a specific local pattern shared by the keypoints in that cluster is saved. Thus, the clustering process generates a visual word vocabulary describing different local patterns in images. The number of clusters determines the size of the Codebook , Hence, each patch in an image is mapped to a certain codeword through the clustering process.



\subsection{Feature Descriptor selection :}
Visual gesture
recognition becomes a research hotspot in machine learning
in recent years, some recognition gesture models consider the
distribution of the palm and fingers structure as the spatial
characteristics of the gesture, which shows robustness and accuracy
when facing image rotation, scale invariance.\\ Geometric
invariant moments are often characterized as an apparent
features of gesture image, but they have to consume a lot of
memory and confront slow calculation and other shortcomings.\\ HOG(histograms of oriented gradients) feature has been introduced into gesture
recognition, but the HOG features is not ideal for gesture
recognition for it is computed in dense grids at some single
scale without orientation alignment.\\ In real world a lot of data are high dimensional that
contains many attributes or characteristics. For the convenience
of representation, usually a data sample is represented as a
vector, each element of the vector correspond to the characteristic
representation.\\ For example, grayscale image data takes
gray value pixel as the feature, according to the order of the
columns, stretchs it into a vector, and then be represented by
a vector image. However, some dimension vectors of image
is very large, i.e. 150 pixels $\times $150 pixel image corresponds
to a high-dimensional space of dimension 22500 this is for One hand dataset made by US \ref{ data1} , As for dataset of Two hands 271 pixels $\times $211 which is   a  space of dimension  of 57181.\\ So Bag of-words
method, which is to select a group of words as a
dictionary, then count the frequency of dictionary word appears
in the image, so that dictionary in reduced dimension space can
indicate the corresponding image.\\ The contribution of this paper is that we propose SURF
feature descriptors of hand gesture images and then bag of
visual words are to map these descriptors to a dimension vector and support vector machine(SVM) classifer is trained to
recognize hand gesture.
\begin{enumerate}
\item We show that this model is not only
effective for common hand gestures dataset, but also achieves
good gesture recognition rate for challenging RGB-D depth
data collected by Kinect sensor.
\item By SURF descriptors, the
dimension of each gesture image is high and can not be directly
used as the input of machine learning algorithms, so Bag Of
Visual Words are used to reduce the dimension. 
\item We found
that SURF feature descriptors are more appropriately for visual
hand gesture recognition than SIFT feature descriptors.
SIFT is  slow which  gave lag during feature extraction. SURF performs well on blurry images and is typically 3 times faster than SIFT.\\ Hence we found SURF descriptors to be apt for our application to meet the demand \textbf{of Real time application} since Sift descriptor can never be used in real time applications however Not with normal machines .
\end{enumerate}

\subsection{Clustering Algorithm Bag of words :}
A texture image with a Bag of Words
model is represented by the image feature,i.e.,a histogram of
the number of texels from a texture image, to generate ”visual
words”, which generated word Code, statistics of each image
frequency of visual words, to complete the image of the word
description of the bag. Fig.\ref{fig:bag} is bag of visual words image
classiﬁcation approach.\\
\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{img/BAG.jpg}
\caption{ bag of visual words image classification approach   }
\label{fig:bag}
\end{figure}
(1) \textbf{Feature extraction and description.} It is to extract a
representative local features from an image as a description
of the image. Traditional methods using SIFT descriptors to
describe the image. This paper uses SURF  descriptor
to extract all the training images feature descriptor. \\(2)\textbf{ Dictionary}
generation. How to generate the visual dictionary in the
entire feature space. After K-means clustering SURF 
feature will be clustered into K categories, the center of each class that is a visual words and give them a characteristic vector of the cluster with the index value. All visual words constitute
a visual dictionary. \\(3)\textbf{ Characteristics of quantization.} During
quantization characteristic, for a given local features of each
image, using the nearest neighbor method to find the distance
to the nearest cluster center feature vectors in the visual
dictionary, the resulting image should be the corresponding
visual words to represent the image of the gesture.\\


\subsection{classification algorithm }
 Support vector machine is one of the mostly
commonly used classiﬁer. The core idea is to ﬁnd the optimal
hyperplane in the feature space.\\
Training Processes are as follows:\\ (1) extracting SURF
and SIFT features from training samples.\\ (2) using K-means
clustering mode to regard cluster center vectors as visual
words.\\ (3) calculating each ﬁgure as training visual saliency
map, and each image in the SIFT features to quantify, and
then based on visual saliency visual computing word weighted
histogram of each graph, and then use the histogram training
image.\\ (4) using SVM to train samples. Each training samples
include positive and negative samples. Positive samples containing
words of visual word histogram, negative samples do
not contain such objects visual word histogram.\\ Classiﬁcation
process:\\ (1) The test image used to extract dense sampling of
images features. \\(2) Calculation of the test image visual words
and quantify the test image, and then calculate the saliency
calculation test image visual word weighted histogram.\\ (3)
using the trained SVM classiﬁer to obtain a classiﬁcation
result.
the following Figure \ref{fig:algo1}  summarize the whole  process :

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{img/myalgo.pdf}
\caption{Flowchart for the structure of the algorithm , in block (A) the Extracted Keypoints  are Clustered to i Vocabulary and generate a codewords dictionary  ,then compute a histogram using visual words ( vocabulary ) for all the training samples these histograms are then fed to an SVM for classification, in Block (B) test images are converted to histogram representation by couting how many keypoints appeared in cluster i ,  and then classify the results using the trained SVM }
\label{fig:algo1}
\end{figure}

\newpage

for Surf Descriptor we have 200 image for each Gesture  which gave approximately 400444 Features to clusters 

\subsection{EXPERIMENTAL RESULTS }
in this section we are interested in maximizing the performances of SVM and KNN , using K-cross Validation to make model selection , using K = 10 for cross Validation which has ensure us a great    ,  . \\
we have chose Confusion to be our main Evaluation Metric since The method can be used for binary classification or Multi class classifiction like Our Case ( 17 class ) 

\subsubsection{ } 

confusion mtrix ...\\
SVM performance ...

\newpage




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{gesture recognition using Fourier Descriptor and NN  classifier }

 The use of depth image has benefit
in segmenting hand image rather than color-based
segmentation, segmentation in depth image is more robust
since the lighting variation does not affect the image quality.
 Fourier descriptors represent feature of each hand gesture as Unique signature As long as the shape (images ) are different from each others .
Generally, this research is separated into two phases:
dictionary build phase and classification phase. \\In the
dictionary build phase, the Fourier descriptors of each
character are stored into a database to develop gesture
dictionary. The gesture dictionary and comparison methods are
derived from \cite{clif}.\\ The classification phase has the similar
step, except that the Fourier descriptors are compared with the
dictionary using distance metric as classification methods.
The result of classification phase is the meaning of the
acquired gesture sign.\\\\
\texttt{
For more details about the algorithm used for this project please check the section \ref{FDT}
}

\subsection{Classification }
Now we have a set of features for each image and the next
step is to train a classifier to classify the different hand shapes
used in testing. the classifier used in for Fourier descriptor is based on Euclidean distance which is a very simple way for
classification.\\
Classification using Euclidean Distance: The set of
features induces a distance on the shape space, which is given
by the Euclidean distance: 
$$d(f_{1},f_{2}) = \sqrt{\sum_{k=0}^{N-1} |f_{1}(k)-f_{2}(k)|^{2}}$$\\
\texttt{ Where $f_{1},f_{2}$ are the feature vectors for the two images being
compared. The two vectors with the least distance will have
the same class}\\



 The following Figure \ref{fig:dft} summarize the whole process :  
        
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{img/latex.jpg}
\caption{the architecture used for FDs}
\label{fig:dft}
\end{figure}


\subsection{EXPERIMENTAL RESULTS}

For each shape we select a set of points with the equal point
sampling method. To use DFT we choose a number that is a
power-of-two. We tested the classifiers with initial 128, 64, 32, 16
and 8 points. Those features are used as a training set to be
used in classification. To classify the test image we used the Classifier Nearest Neighbor that uses simple  Euclidean distance , where we calculated the distance between the features of each image in
the test set with the features of the Dictionary .

%recognition rate for each classifier using the formula in \ref{equa:1}.\\
%\begin{gather}
 %   Recognition\ rate = \frac{the\ number\ of\ recognized\ images}{the\ number\ of\ %testing\ images} 
  %  \label{equa:1}
%\end{gather}